Steps,Policy/Entropy,Environment/Episode Length,Policy/Extrinsic Value Estimate,Environment/Cumulative Reward,Policy/Extrinsic Reward,Losses/Value Loss,Losses/Policy Loss,Policy/Learning Rate,Is Training
10000,1.0564857,4.896876841484973,-0.32883123,-0.7553065982032215,-0.7553065982032215,0.23221517,0.068063475,0.00029996925,1.0
20000,0.7948792,10.067552602436324,-0.5852186,-0.49684384699543716,-0.49684384699543716,0.08548823,0.07030543,0.00029991375,1.0
30000,0.75838906,22.535211267605632,-0.32462442,0.11772301086517567,0.11772301086517567,0.19548485,0.068718866,0.00029985193,1.0
40000,0.72960794,58.127272727272725,0.34653127,1.9162651830049882,1.9162651830049882,0.33025074,0.0642548,0.0002997897,1.0
50000,0.6783193,178.8,1.3743225,7.940000653266907,7.940000653266907,0.3078115,0.065397404,0.00029972752,1.0
60000,0.6197844,462.09090909090907,2.3627179,22.104547358371995,22.104547358371995,0.2414829,0.06646914,0.0002996654,1.0
70000,0.57742745,498.2,3.155196,23.91000206172466,23.91000206172466,0.3006194,0.0657363,0.00029960932,1.0
80000,0.5583807,673.1333333333333,3.6172485,32.65666955312093,32.65666955312093,0.26806903,0.065218225,0.00029955315,1.0
