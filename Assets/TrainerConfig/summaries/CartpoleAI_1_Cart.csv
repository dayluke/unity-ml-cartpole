Steps,Policy/Entropy,Environment/Episode Length,Policy/Extrinsic Value Estimate,Environment/Cumulative Reward,Policy/Extrinsic Reward,Losses/Value Loss,Losses/Policy Loss,Policy/Learning Rate,Is Training
10000,1.0647764,4.763688760806916,-0.3573982,-0.7617647003859385,-0.7617647003859385,0.22824141,0.07241039,0.00029996925,1.0
20000,0.82149357,10.362088535754824,-0.6019092,-0.48240635533067616,-0.48240635533067616,0.08939528,0.07054224,0.00029991375,1.0
30000,0.75495064,26.83008356545961,-0.22666723,0.3328691016166655,0.3328691016166655,0.23578915,0.06654091,0.00029985158,1.0
40000,0.66970843,119.1566265060241,0.6576583,4.930952766111919,4.930952766111919,0.3277654,0.065697804,0.00029978948,1.0
50000,0.572402,279.3142857142857,1.7666608,12.965715397255762,12.965715397255762,0.29376155,0.064397424,0.0002997275,1.0
60000,0.5464692,465.3125,2.6273088,22.265626894310117,22.265626894310117,0.20457351,0.06719603,0.0002996652,1.0
70000,0.53360194,1210.8,3.4795387,59.540005242824556,59.540005242824556,0.1595833,0.067821845,0.00029960912,1.0
80000,0.54084176,2043.5,4.0617914,101.17500899732113,101.17500899732113,0.11542877,0.06699524,0.0002995532,1.0
90000,0.517642,5040.0,4.436446,251.00002241134644,251.00002241134644,0.026687557,0.068786204,0.0002994903,1.0
100000,0.5376501,10498.0,4.67523,523.9000469446182,523.9000469446182,0.0054046037,0.06986501,0.00029942725,1.0
110000,0.52604663,None,4.826322,None,None,0.0006654821,0.06473575,0.0002993703,1.0
